{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 并行网页抓取\n",
    "1. 从多个数据源(多个远程服务器)而不只是一个数据源收集数据;\n",
    "2. 收集数据的同时，在已收集到的数据上执行时间更长 / 更复杂的操作(例如图像分析或OCR处理)\n",
    "3. 从大型 Web 服务收集数据，如果你已经付费，或者创建多个连接是使用协议允许的行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Python 既支持多进程(multiprocessing)，也支持多线程(multithreading)。多进程和多线程 可以实现相同的目标:同时执行两个编程任务，而不是像传统线性方式那样一次只执行一 个任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Counter\n",
      "2Counter\n",
      "3Fizz\n",
      "3Counter\n",
      "4Counter\n",
      "5Buzz\n",
      "5Counter\n",
      "6Fizz\n",
      "6Counter\n",
      "7Counter\n",
      "8Counter\n",
      "9Fizz\n",
      "9Counter\n",
      "10Buzz\n",
      "10Counter\n",
      "11Counter\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError unable to start thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''多线程爬取'''\n",
    "import _thread\n",
    "import time \n",
    "\n",
    "def print_time(threadName,delay,iterations):\n",
    "    start = int(time.time())\n",
    "    for i in range(0,iterations):\n",
    "        time.sleep(delay)\n",
    "        seconds_elapsed = str(int(time.time()) - start)\n",
    "        print(\"{}{}\".format(seconds_elapsed,threadName))\n",
    "\n",
    "'''这个脚本开启了 3 个线程:\n",
    "    一个线程每 3 秒打印一次“Fizz”\n",
    "    另一个线程每 5 秒打印一次\"Buzz”\n",
    "    第三个线程每秒打印一次“Counter”\n",
    "'''\n",
    "try:\n",
    "    _thread.start_new_thread(print_time,(\"Fizz\",3,33))\n",
    "    _thread.start_new_thread(print_time,(\"Buzz\",5,20))\n",
    "    _thread.start_new_thread(print_time,(\"Counter\",1,100))\n",
    "except:\n",
    "    print(\"Error unable to start thread\")\n",
    "while 1:\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping Kevin Bacon in thread Thread 1\n",
      "getting links in Thread 1\n",
      "/wiki/David_Koepp\n",
      "scraping Monty Python in thread Thread 2\n",
      "getting links in Thread 2\n",
      "/wiki/Michael_Palin\n",
      "scraping David Koepp in thread Thread 1\n",
      "getting links in Thread 1\n",
      "/wiki/Empire_(film_magazine)\n",
      "scraping Michael Palin in thread Thread 2\n",
      "getting links in Thread 2\n",
      "/wiki/Marriage_Guidance_Counsellor\n",
      "scraping Empire (magazine) in thread Thread 1\n",
      "getting links in Thread 1\n",
      "/wiki/Empire_Awards\n",
      "scraping Marriage Guidance Counsellor in thread Thread 2\n",
      "getting links in Thread 2\n",
      "/wiki/Michael_Palin\n",
      "scraping Empire Awards in thread Thread 1\n",
      "getting links in Thread 1\n",
      "/wiki/Empire_Inspiration_Award\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#W4sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39merror unable to start thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#W4sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''多线程爬取网站'''\n",
    "from os import link\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import random\n",
    "\n",
    "import _thread\n",
    "import time \n",
    "visited = []\n",
    "\n",
    "def get_links(thread_name,bs):\n",
    "    print(\"getting links in {}\".format(thread_name))\n",
    "    links =  bs.find(\"div\",{\"id\":\"bodyContent\"}).find_all(\"a\",href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "    return [link for link in links if link not in visited]\n",
    "\n",
    "# 为线程定义一个函数\n",
    "def sracpe_article(thread_name,path):\n",
    "    '''需要注意的是，现在 scrape_article 函数的第一个动作，\n",
    "    是把当前网页的路径添加到已经 浏览过的路径列表中。\n",
    "    这会减小抓取两次的可能性，但是不会彻底解决这类问题。'''\n",
    "    visited.append(path)\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "    time.sleep(5)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.find(\"h1\").get_text()\n",
    "    print(\"scraping {} in thread {}\".format(title,thread_name) )\n",
    "    links = get_links(thread_name,bs)\n",
    "    if len(links)>0:\n",
    "        newArticle = links[random.randint(0,len(links) -1 )].attrs['href']\n",
    "        print(newArticle)\n",
    "        sracpe_article(thread_name,newArticle)\n",
    "\n",
    "# 创建两个线程\n",
    "try:\n",
    "    _thread.start_new_thread(sracpe_article, ('Thread 1', '/wiki/Kevin_Bacon',))\n",
    "    _thread.start_new_thread(sracpe_article, ('Thread 2', '/wiki/Monty_Python',))\n",
    "except:\n",
    "    print(\"error unable to start thread\")\n",
    "while 1:\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Kevin Bacon for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 Kevin Bacon\n",
      "Added Monty Python for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "存储文章 Monty Python\n",
      "Added Apollo 13 (film) for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 Apollo 13 (film)\n",
      "Added Joan Miró for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "存储文章 Joan Miró\n",
      "Added Max Elliott Slade for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 Max Elliott Slade\n",
      "Added Wayback Machine for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "存储文章 Wayback Machine\n",
      "Added Parenthood (1990 TV series) for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 Parenthood (1990 TV series)\n",
      "Added Time capsule for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "存储文章 Time capsule\n",
      "Added Jasen Fisher for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 Jasen Fisher\n",
      "Added George Edward Pendray for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "存储文章 George Edward Pendray\n",
      "Added The Witches (1990 film) for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 The Witches (1990 film)\n",
      "Added Spaceflight for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "存储文章 Spaceflight\n",
      "Added Bambaloo for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 Bambaloo\n",
      "Added Organization for Security and Co-operation in Europe for storage in thread Thread 2Added Jim Henson Company Lot for storage in thread Thread 1\n",
      "\n",
      "获取链接Thread 1\n",
      "获取链接Thread 2\n",
      "存储文章 Jim Henson Company Lot\n",
      "存储文章 Organization for Security and Co-operation in Europe\n",
      "Added Turkey Hollow for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 Turkey Hollow\n",
      "Added Wayback Machine for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "文章已经存在Wayback Machine\n",
      "Added Bibliotheca Alexandrina for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "存储文章 Bibliotheca Alexandrina\n",
      "Added Nova Scotia for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 Nova Scotia\n",
      "Added Virtual International Authority File for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "存储文章 Virtual International Authority File\n",
      "Added Mainland for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 Mainland\n",
      "Added Japanese language for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "存储文章 Japanese language\n",
      "Added Danish Realm for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 Danish Realm\n",
      "Added Timeline of Greenland for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "Added Gojūon for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "存储文章 Timeline of Greenland\n",
      "存储文章 Gojūon\n",
      "Added So (kana) for storage in thread Thread 2\n",
      "获取链接Thread 2\n",
      "存储文章 So (kana)\n",
      "Added Nuclear weapon for storage in thread Thread 1\n",
      "获取链接Thread 1\n",
      "存储文章 Nuclear weapon\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39merror:unable to start threads\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#W5sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#W5sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''使用队列数据结构来爬取网站\n",
    "可以采用较少的数据库线程，每个线程都有独立的连接，从队列来回获取并存储 数据。这样可以实现更加可控的数据库连接\n",
    "'''\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import random\n",
    "import _thread\n",
    "from queue import Queue\n",
    "import time \n",
    "import pymysql\n",
    "\n",
    "\n",
    "def storage(quene):\n",
    "    conn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',user='root', passwd='2022mysql', db='mysql', charset='utf8')\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"use scraping\")\n",
    "    while 1:\n",
    "        if not quene.empty():\n",
    "            article = quene.get()\n",
    "            cur.execute(\"select * from wiki_pages where path = %s\",(article[\"path\"]))\n",
    "            if cur.rowcount==0:\n",
    "                print(\"存储文章 {}\".format(article[\"title\"]))\n",
    "                cur.execute(\"insert into wiki_pages (title,path) values(%s,%s)\",(article['title'],article['path']))\n",
    "                conn.commit()\n",
    "            else:\n",
    "                print(\"文章已经存在{}\".format(article['title']))\n",
    "\n",
    "\n",
    "visited = []\n",
    "def getlinks(thread_name,bs):\n",
    "    print(\"获取链接{}\".format(thread_name))\n",
    "    links = bs.find('div', {'id':'bodyContent'}).find_all('a',href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "    return [link for link in links if link not in visited]\n",
    "\n",
    "def scrape_article(thread_name,path,queue):\n",
    "    visited.append(path)\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "    time.sleep(5)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    title = bs.find('h1').get_text()\n",
    "    print('Added {} for storage in thread {}'.format(title, thread_name))\n",
    "    queue.put({\"title\":title, \"path\":path})\n",
    "    links = getlinks(thread_name, bs)\n",
    "\n",
    "    if len(links)>0:\n",
    "        newArticle = links[random.randint(0,len(links)-1)].attrs['href']\n",
    "        scrape_article(thread_name,newArticle,queue)\n",
    "\n",
    "queue = Queue()\n",
    "try:\n",
    "    _thread.start_new_thread(scrape_article, ('Thread 1','/wiki/Kevin_Bacon', queue,))\n",
    "    _thread.start_new_thread(scrape_article, ('Thread 2','/wiki/Monty_Python', queue,))\n",
    "    _thread.start_new_thread(storage, (queue,))\n",
    "except:\n",
    "    print(\"error:unable to start threads\")\n",
    "while 1:\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    " threading 模块是 一个高级接口，可以让你轻松地使用线程，同时也暴露了 _thread 模块的所有特性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time \n",
    "def print_time(threadName,delay,iterations):\n",
    "    start = int(time.time())\n",
    "    for i in range(0,iterations):\n",
    "        time.sleep(delay)\n",
    "        seconds_elapsed = str(int(time.time()) - start)\n",
    "        print(\"{}{}\".format(seconds_elapsed,threadName))\n",
    "\n",
    "threading.Thread(target=print_time, args=('Fizz', 3, 33)).start()\n",
    "threading.Thread(target=print_time, args=('Buzz', 5, 20)).start()\n",
    "threading.Thread(target=print_time, args=('Counter', 1, 100)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**threading 模块的一个优点是，它可以轻松地创建其他线程都无法访问的线程局部数据 (local thread data)。这样做的好处是，如果你有若干线程，它们各自抓取不同的网站，那\n",
    "么每个线程都可以跟踪自己访问的页面列表。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''局部数据可以随时创建，调用线程函数即可'''\n",
    "from ast import arg\n",
    "import threading\n",
    "def crawler(url):\n",
    "    data = threading.local()\n",
    "    data.visited = []\n",
    "    # 抓取网站\n",
    "threading.Thread(target=crawler, args=('http://brookings.edu')).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样就可以解决线程之间因为共享对象而<u>导致竞争条件</u>的问题。无论何时，只要不需要共享对象，就不要共享，保存在线程局部内存中即可。为了安全地在线程中共享对象，仍然 可以使用上一节中的 Queue 模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Crawler' object has no attribute 'isAlive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39;49misAlive():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     t \u001b[39m=\u001b[39m Crawler()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danyow/Desktop/extra-learning/spyier/chapter16-process.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     t\u001b[39m.\u001b[39mstart()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Crawler' object has no attribute 'isAlive'"
     ]
    }
   ],
   "source": [
    "'''通常情况下，爬虫都需要运行很长时间。isAlive 函数可以确保爬虫在一个线程崩溃后重启:'''\n",
    "import threading\n",
    "import time \n",
    "class Crawler(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.done = False\n",
    "    def isDone(self):\n",
    "        return self.done \n",
    "    def run(self):\n",
    "        time.sleep(5)\n",
    "        self.done = True\n",
    "        raise Exception(\"something bad happebd!\")\n",
    "\n",
    "    \n",
    "t = Crawler()\n",
    "t.start()\n",
    "\n",
    "while True:\n",
    "    time.sleep(1)\n",
    "    if t.isDone():\n",
    "        print('Done')\n",
    "        break\n",
    "    if not t.isAlive():\n",
    "        t = Crawler()\n",
    "        t.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "# 多进程抓取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import time \n",
    "\n",
    "def print_time(threadName,delay,iterations):\n",
    "    start = int(time.time())\n",
    "    for i in range(0,iterations):\n",
    "        time.sleep(delay)\n",
    "        seconds_elapsed = str(int(time.time()) - start)\n",
    "        print (threadName if threadName else seconds_elapsed)\n",
    "processes = []\n",
    "processes.append(Process(target=print_time, args=('Counter', 1, 100)))\n",
    "processes.append(Process(target=print_time, args=('Fizz', 3, 33)))\n",
    "processes.append(Process(target=print_time, args=('Buzz', 5, 20)))\n",
    "for p in processes:\n",
    "    p.start()\n",
    "for p in processes:\n",
    "    p.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**多进程抓取网页**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'scrape_article' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'scrape_article' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import random\n",
    "from multiprocessing import Process\n",
    "import os \n",
    "import time \n",
    "visited = []\n",
    "def get_links(bs):\n",
    "    print('Getting links in {}'.format(os.getpid()))\n",
    "    links = bs.find('div', {'id':'bodyContent'}).find_all('a',\n",
    "             href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "    return [link for link in links if link not in visited]\n",
    "\n",
    "\n",
    "def scrape_article(path):\n",
    "    visited.append(path)\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "    time.sleep(5)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.find('h1').get_text()\n",
    "    print('Scraping {} in process {}'.format(title, os.getpid()))\n",
    "\n",
    "    links = get_links(bs)\n",
    "    if len(links)>0:\n",
    "        newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "        print(newArticle)\n",
    "        scrape_article(newArticle)\n",
    "\n",
    "processes = []\n",
    "processes.append(Process(target=scrape_article, args=('/wiki/Kevin_Bacon',)))\n",
    "processes.append(Process(target=scrape_article, args=('/wiki/Monty_Python',)))\n",
    "for p in processes:\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**有一种方法可以让同一台机器上的进程互相通信，那就是用 Python 的两个对象:队 列和管线(pipe)。**\n",
    "如果将网页的静态列表替换成某种抓取委托器(delegator)会怎样呢?爬虫以待抓取网页 路径的形式(例如 /wiki/Monty_Python)，从一个队列中获取一个任务，抓取结束后再将一 个“已发现 URL”的列表返回到另一个独立的队列中，这个队列将由抓取委托器来处理， 这样就只有新的 URL 会被添加到第一个任务队列中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "        exitcode = _main(fd, parent_sentinel)exitcode = _main(fd, parent_sentinel)\n",
      "      File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError:     self = reduction.pickle.load(from_parent)\n",
      "Can't get attribute 'scrape_article' on <module '__main__' (built-in)>AttributeError: \n",
      "Can't get attribute 'task_delegator' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'scrape_article' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from multiprocessing import Process, Queue\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def task_delegator(taskQueue,urlsQueue):\n",
    "    # 为每个进程初始化一个任务\n",
    "    visited = ['/wiki/Kevin_Bacon', '/wiki/Monty_Python']\n",
    "    taskQueue.put('/wiki/Kevin_Bacon')\n",
    "    taskQueue.put('/wiki/Monty_Python')\n",
    "\n",
    "    while 1:\n",
    "        # 检查urlsQueue中是否存在新链接需要处理\n",
    "        if not urlsQueue.empty():\n",
    "            links = [link for link in urlsQueue.get() if link not in visited]\n",
    "            for link in links:\n",
    "                # 向taskQueue中增加新链接\n",
    "                taskQueue.put(link)\n",
    "\n",
    "def get_links(bs):\n",
    "    links = bs.find('div', {'id':'bodyContent'}).find_all('a',\n",
    "             href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "    return [link.attrs['href'] for link in links]\n",
    "\n",
    "\n",
    "def scrape_article(taskQueue,urlsQueue):\n",
    "    while 1:\n",
    "        while taskQueue.empty():\n",
    "            # 如果任务队列为空，休息100毫秒\n",
    "            # 这种情况应该极少发生\n",
    "            time.sleep(.1)\n",
    "        path = taskQueue.get()\n",
    "        html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "        time.sleep(5)\n",
    "        bs = BeautifulSoup(html, 'html.parser')\n",
    "        title = bs.find('h1').get_text()\n",
    "        print('Scraping {} in process {}'.format(title, os.getpid()))\n",
    "        links = get_links(bs)\n",
    "        # 发送这些链接到委托器进行处理\n",
    "        urlsQueue.put(links)\n",
    "processes = []\n",
    "taskQueue = Queue()\n",
    "urlsQueue = Queue()\n",
    "processes.append(Process(target=task_delegator, args=(taskQueue, urlsQueue,)))\n",
    "processes.append(Process(target=scrape_article, args=(taskQueue, urlsQueue,)))\n",
    "processes.append(Process(target=scrape_article, args=(taskQueue, urlsQueue,)))\n",
    "for p in processes:\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('myenvs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a051216bf3a65c020b643b52ab15d0714db61718a99958c54a11e422694469a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
